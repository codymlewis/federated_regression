{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Exacerbation of Permutation Invariance and Overfitting in Federated Learning of Regression Tasks\n",
    "\n",
    "In this notebook, we demonstrate that when performing federated learning of regression tasks, the issues of permutation\n",
    "invariance and client-level overfitting are significantly more prominent. We observe that the fact that regression uses\n",
    "real outputs leads significant variance in client's optimal weights since there are infinitely many weighted sums that\n",
    "can lead to the correct output. In federated learning, clients hold non-i.i.d. datasets to eachother and may perform\n",
    "many steps of model updates before synchronising with the rest of the system, this results in client models that may\n",
    "individually fit well to a subset of the data but are so significantly different in weights to eachother that the aggregated\n",
    "model does not fit well to any of the data. We call this effect client-level overfitting.\n",
    "\n",
    "To start our demonstration we import the required external libraries, and establish constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Tuple, List, Callable\n",
    "from numpy.typing import NDArray, ArrayLike\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn.datasets as skd\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import trange\n",
    "\n",
    "\n",
    "seed = round(np.pi**13 + np.e * 21)\n",
    "tf.random.set_seed(seed)\n",
    "rng = np.random.default_rng(seed)\n",
    "batch_size = 200\n",
    "num_epochs = 25\n",
    "num_clients = 10\n",
    "num_rounds = num_epochs * num_clients\n",
    "\n",
    "def evaluate(Y_test: NDArray, preds: ArrayLike):\n",
    "    print(f\"R2 score: {skm.r2_score(Y_test, preds)}\")\n",
    "    print(f\"MAE: {skm.mean_absolute_error(Y_test, preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the California housing dataset, which is a regression task where information about a house and its area is used to predict its\n",
    "price in units of hundreds of thousands of dollars. The dataset is automatically attained from https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html.\n",
    "This dataset allows us to present the simplest format of this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = skd.fetch_california_housing(return_X_y=True)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralised Machine Learning\n",
    "\n",
    "First, we will show that an equivalent centralised machine learning setting can effectively learn the task.\n",
    "\n",
    "For our model, we use a simple multilayer perceptron. We can see that it attains good performance when trained locally on the entire training dataset with the Adam optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.5447022527398048\n",
      "MAE: 0.5550954728767437\n"
     ]
    }
   ],
   "source": [
    "def create_model(input_shape: Tuple[int]):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    x = inputs\n",
    "    x = tf.keras.layers.Dense(100, activation=\"sigmoid\")(x)\n",
    "    x = tf.keras.layers.Dense(1, activation=\"relu\")(x)  # All labels are positive\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=\"mean_absolute_error\")\n",
    "    return model\n",
    "\n",
    "model = create_model(X_train[0].shape)\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, verbose=0)\n",
    "evaluate(Y_test, model.predict(X_test, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning\n",
    "\n",
    "Now, we create a similar setting, but within federated learning. There is one small difference in our setting, data is distributed in a non-i.i.d. manner\n",
    "across clients.\n",
    "\n",
    "With that said, we first define a structure to hold each client's data, and a function to distribute the data to each client. The distribution function\n",
    "is made non-i.i.d. by assigning each client and equidistant mean label and giving them all samples that have labels that are nearest to that mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClientData(NamedTuple):\n",
    "    X: NDArray\n",
    "    Y: NDArray\n",
    "\n",
    "\n",
    "def distribute_client_data(X_train: NDArray, Y_train: NDArray):\n",
    "    hist, bins = np.histogram(Y_train, 9)\n",
    "    bin_idx = np.digitize(Y_train, bins) - 1\n",
    "    all_client_data = []\n",
    "    for c in range(num_clients):\n",
    "        idx = bin_idx == c\n",
    "        all_client_data.append(ClientData(X_train[idx].copy(), Y_train[idx].copy()))\n",
    "    return all_client_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define our federated optimisers: federated averaging and FedAdam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedavg(\n",
    "    client_parameters: List[List[NDArray]],\n",
    "    client_samples: List[int],\n",
    "    *_args\n",
    "):\n",
    "    agg_parameters = []\n",
    "    num_layers = len(client_parameters[0])\n",
    "    for i in range(num_layers):\n",
    "        agg_parameters.append(\n",
    "            np.average([cp[i] for cp in client_parameters], weights=client_samples, axis=0)\n",
    "        )\n",
    "    return agg_parameters\n",
    "\n",
    "\n",
    "class FedAdam:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.01,\n",
    "        beta_1: float = 0.9,\n",
    "        beta_2: float = 0.999,\n",
    "        epsilon: float = 1e-07\n",
    "    ):\n",
    "        self.mu = None\n",
    "        self.nu = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        client_parameters: List[List[NDArray]],\n",
    "        client_samples: List[int],\n",
    "        global_parameters: List[NDArray]\n",
    "    ):\n",
    "        if self.mu is None:\n",
    "            self.mu = [np.zeros_like(p) for p in global_parameters]\n",
    "            self.nu = [np.zeros_like(p) for p in global_parameters]\n",
    "\n",
    "        client_grads = [\n",
    "            [cp - gp for cp, gp in zip(cparams, global_parameters)] for cparams in client_parameters\n",
    "        ]\n",
    "        agg_grads = fedavg(client_grads, client_samples)\n",
    "        for i in range(len(global_parameters)):\n",
    "            self.mu[i] = self.beta_1 * self.mu[i] + (1 - self.beta_1) * agg_grads[i]\n",
    "            self.nu[i] = self.beta_2 * self.nu[i] + (1 - self.beta_2) * agg_grads[i]**2\n",
    "            mu_hat = self.mu[i] / (1 - self.beta_1)\n",
    "            nu_hat = self.nu[i] / (1 - self.beta_2)\n",
    "            layer_update = mu_hat / (np.sqrt(nu_hat) + self.epsilon)\n",
    "            global_parameters[i] += self.learning_rate * layer_update\n",
    "        return global_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we define a function to perform our federated learning experiment by creating the clients, having them train for each round, aggregate the client models\n",
    "using the specified optimizer at the end of each round. When learning is completed, we print the $R^2$ score and mean absolute error of the learnt global model evaluated\n",
    "on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_learning(\n",
    "    create_model_fun: Callable[[Tuple[int]], tf.keras.Model],\n",
    "    optimiser: Callable[[List[List[NDArray]], List[int], List[NDArray]], List[NDArray]],\n",
    "    all_client_data: List[ClientData]\n",
    "):\n",
    "    global_model = create_model(X_train[0].shape)\n",
    "    client_models = [create_model(X_train[0].shape) for _ in range(num_clients)]\n",
    "    num_client_samples = [len(cd.Y) for cd in all_client_data]\n",
    "\n",
    "    for r in (pbar := trange(num_rounds)):\n",
    "        losses = []\n",
    "        for client_model, client_data in zip(client_models, all_client_data):\n",
    "            client_model.set_weights(global_model.get_weights())\n",
    "            history = client_model.fit(client_data.X, client_data.Y, batch_size=batch_size, verbose=0)\n",
    "            losses.append(history.history['loss'][0])\n",
    "        global_model.set_weights(\n",
    "            optimiser([cm.get_weights() for cm in client_models], num_client_samples, global_model.get_weights())\n",
    "        )\n",
    "        pbar.set_postfix_str(f\"Loss: {np.average(losses, weights=num_client_samples):.3f}\")\n",
    "\n",
    "    evaluate(Y_test, global_model.predict(X_test, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now set up for the experiments, first we demonstrate with the same model as the centralised training and federated averaging as\n",
    "the federated optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 250/250 [01:31<00:00,  2.74it/s, Loss: 0.741]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: -0.126455588983152\n",
      "MAE: 0.8951023149588125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "federated_learning(create_model, fedavg, distribute_client_data(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this form of training is completely ineffective, so we try a different federated optimiser, FedAdam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 250/250 [01:32<00:00,  2.70it/s, Loss: 2.070]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: -3.110291978270628\n",
      "MAE: 2.0622381468023256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "federated_learning(create_model, FedAdam(), distribute_client_data(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It remains arguable that this effect could simple by a side effect of the client-side Adam optimiser being used in training, and the updates\n",
    "it produces with have differing denominators causing incorrect aggregation. However, with the following we construct a closer replication of\n",
    "the centralised setting by applying the SGD optimiser at the client-side and FedAdam at the server. Still, we see that client-level\n",
    "overfitting has a significant negative impact on the resulting global model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 250/250 [01:32<00:00,  2.70it/s, Loss: 0.634]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: -0.1577248615841711\n",
      "MAE: 0.9079689140678654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_sgd_model(input_shape: Tuple[int]):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    x = inputs\n",
    "    x = tf.keras.layers.Dense(100, activation=\"sigmoid\")(x)\n",
    "    x = tf.keras.layers.Dense(1, activation=\"relu\")(x)  # All labels are positive\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(0.1), loss=\"mean_absolute_error\")\n",
    "    return model\n",
    "\n",
    "federated_learning(create_sgd_model, FedAdam(), distribute_client_data(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With i.i.d. Data\n",
    "\n",
    "With the following we show that this effect is not present when the data is i.i.d. with respect to the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 250/250 [01:32<00:00,  2.70it/s, Loss: 0.501]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.6462583840270035\n",
      "MAE: 0.4894915336543876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def iid_client_data(X_train: NDArray, Y_train: NDArray):\n",
    "    all_idx = np.arange(len(Y_train))\n",
    "    rng.shuffle(all_idx)\n",
    "    client_idxs = np.split(all_idx[:-(num_clients - len(Y_test) % num_clients)], num_clients)\n",
    "    all_client_data = []\n",
    "    for cidx in client_idxs:\n",
    "        all_client_data.append(ClientData(X_train[cidx].copy(), Y_train[cidx].copy()))\n",
    "    return all_client_data\n",
    "\n",
    "federated_learning(create_model, fedavg, iid_client_data(X_train, Y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
