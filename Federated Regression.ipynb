{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Exacerbation of Permutation Invariance and Overfitting in Federated Learning of Regression Tasks\n",
    "\n",
    "In this notebook, we demonstrate that when performing federated learning of regression tasks, the issues of permutation\n",
    "invariance and client-level overfitting are significantly more prominent. We observe that the fact that regression uses\n",
    "real outputs leads significant variance in client's optimal weights since there are infinitely many weighted sums that\n",
    "can lead to the correct output. In federated learning, clients hold non-i.i.d. datasets to eachother and may perform\n",
    "many steps of model updates before synchronising with the rest of the system, this results in client models that may\n",
    "individually fit well to a subset of the data but are so significantly different in weights to eachother that the aggregated\n",
    "model does not fit well to any of the data. We call this effect client-level overfitting.\n",
    "\n",
    "To start our demonstration we import the required external libraries, and establish constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 15:27:54.188501: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-07 15:27:54.215139: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-07 15:27:54.215559: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-07 15:27:54.832689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple, Tuple, List, Callable\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "from numpy.typing import NDArray, ArrayLike\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn.datasets as skd\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.optimize as sp_opt\n",
    "from tqdm.auto import trange\n",
    "\n",
    "\n",
    "seed = round(np.pi**13 + np.e * 21)\n",
    "tf.random.set_seed(seed)\n",
    "rng = np.random.default_rng(seed)\n",
    "batch_size = 200\n",
    "num_epochs = 25\n",
    "num_clients = 10\n",
    "num_rounds = num_epochs * num_clients\n",
    "\n",
    "def evaluate(Y_test: NDArray, preds: ArrayLike):\n",
    "    print(f\"R2 score: {skm.r2_score(Y_test, preds)}\")\n",
    "    print(f\"MAE: {skm.mean_absolute_error(Y_test, preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the California housing dataset, which is a regression task where information about a house and its area is used to predict its\n",
    "price in units of hundreds of thousands of dollars. The dataset is automatically attained from https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html.\n",
    "This dataset allows us to present the simplest format of this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = skd.fetch_california_housing(return_X_y=True)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralised Machine Learning\n",
    "\n",
    "First, we will show that an equivalent centralised machine learning setting can effectively learn the task.\n",
    "\n",
    "For our model, we use a simple multilayer perceptron. We can see that it attains good performance when trained locally on the entire training dataset with the Adam optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.544383624093583\n",
      "MAE: 0.545953413051085\n"
     ]
    }
   ],
   "source": [
    "def create_model(input_shape: Tuple[int]):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    x = inputs\n",
    "    x = tf.keras.layers.Dense(100, activation=\"sigmoid\")(x)\n",
    "    x = tf.keras.layers.Dense(1, activation=\"relu\")(x)  # All labels are positive\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=\"mean_absolute_error\")\n",
    "    return model\n",
    "\n",
    "model = create_model(X_train[0].shape)\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, verbose=0)\n",
    "evaluate(Y_test, model.predict(X_test, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning\n",
    "\n",
    "Now, we create a similar setting, but within federated learning. There is one small difference in our setting, data is distributed in a non-i.i.d. manner\n",
    "across clients.\n",
    "\n",
    "With that said, we first define a structure to hold each client's data, and a function to distribute the data to each client. The distribution function\n",
    "is made non-i.i.d. by assigning each client and equidistant mean label and giving them all samples that have labels that are nearest to that mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClientData(NamedTuple):\n",
    "    X: NDArray\n",
    "    Y: NDArray\n",
    "\n",
    "\n",
    "def non_iid_client_data(X_train: NDArray, Y_train: NDArray):\n",
    "    hist, bins = np.histogram(Y_train, 9)\n",
    "    bin_idx = np.digitize(Y_train, bins) - 1\n",
    "    all_client_data = []\n",
    "    for c in range(num_clients):\n",
    "        idx = bin_idx == c\n",
    "        all_client_data.append(ClientData(X_train[idx].copy(), Y_train[idx].copy()))\n",
    "    return all_client_data\n",
    "\n",
    "\n",
    "def iid_client_data(X_train: NDArray, Y_train: NDArray):\n",
    "    data_per_client = len(Y_test) // num_clients\n",
    "    all_client_data = []\n",
    "    for c in range(num_clients):\n",
    "        cidx = rng.choice(len(Y_train), data_per_client, replace=False)\n",
    "        all_client_data.append(ClientData(X_train[cidx].copy(), Y_train[cidx].copy()))\n",
    "        X_train, Y_train = np.delete(X_train, cidx, axis=0), np.delete(Y_train, cidx, axis=0)\n",
    "    return all_client_data\n",
    "\n",
    "\n",
    "def mixed_iid_client_data(X_train: NDArray, Y_train: NDArray, proportion_iid: float = 0.5):\n",
    "    iid_idx = rng.choice(len(Y_train), round(len(Y_train) * proportion_iid), replace=False)\n",
    "    all_iid_data = iid_client_data(X_train[iid_idx].copy(), Y_train[iid_idx].copy())\n",
    "    all_non_iid_data = non_iid_client_data(np.delete(X_train.copy(), iid_idx, axis=0), np.delete(Y_train.copy(), iid_idx, axis=0))\n",
    "    all_client_data = [\n",
    "        ClientData(np.concatenate((iidd.X, niidd.X)), np.concatenate((iidd.Y, niidd.Y)))\n",
    "        for iidd, niidd in zip(all_iid_data, all_non_iid_data)\n",
    "    ]\n",
    "    return all_client_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define our federated optimisers: federated averaging and FedAdam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedavg(\n",
    "    client_parameters: List[List[NDArray]],\n",
    "    client_samples: List[int],\n",
    "    *_args\n",
    "):\n",
    "    agg_parameters = []\n",
    "    num_layers = len(client_parameters[0])\n",
    "    for i in range(num_layers):\n",
    "        agg_parameters.append(\n",
    "            np.average([cp[i] for cp in client_parameters], weights=client_samples, axis=0)\n",
    "        )\n",
    "    return agg_parameters\n",
    "\n",
    "\n",
    "class FedAdam:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.01,\n",
    "        beta_1: float = 0.9,\n",
    "        beta_2: float = 0.999,\n",
    "        epsilon: float = 1e-07\n",
    "    ):\n",
    "        self.mu = None\n",
    "        self.nu = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        client_parameters: List[List[NDArray]],\n",
    "        client_samples: List[int],\n",
    "        global_parameters: List[NDArray]\n",
    "    ):\n",
    "        if self.mu is None:\n",
    "            self.mu = [np.zeros_like(p) for p in global_parameters]\n",
    "            self.nu = [np.zeros_like(p) for p in global_parameters]\n",
    "\n",
    "        client_grads = [\n",
    "            [cp - gp for cp, gp in zip(cparams, global_parameters)] for cparams in client_parameters\n",
    "        ]\n",
    "        agg_grads = fedavg(client_grads, client_samples)\n",
    "        for i in range(len(global_parameters)):\n",
    "            self.mu[i] = self.beta_1 * self.mu[i] + (1 - self.beta_1) * agg_grads[i]\n",
    "            self.nu[i] = self.beta_2 * self.nu[i] + (1 - self.beta_2) * agg_grads[i]**2\n",
    "            mu_hat = self.mu[i] / (1 - self.beta_1)\n",
    "            nu_hat = self.nu[i] / (1 - self.beta_2)\n",
    "            layer_update = mu_hat / (np.sqrt(nu_hat) + self.epsilon)\n",
    "            global_parameters[i] += self.learning_rate * layer_update\n",
    "        return global_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we define a function to perform our federated learning experiment by creating the clients, having them train for each round, aggregate the client models\n",
    "using the specified optimizer at the end of each round. When learning is completed, we print the $R^2$ score and mean absolute error of the learnt global model evaluated\n",
    "on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_learning(\n",
    "    create_model_fun: Callable[[Tuple[int]], tf.keras.Model],\n",
    "    optimiser: Callable[[List[List[NDArray]], List[int], List[NDArray]], List[NDArray]],\n",
    "    all_client_data: List[ClientData]\n",
    "):\n",
    "    global_model = create_model_fun(X_train[0].shape)\n",
    "    client_models = [create_model_fun(X_train[0].shape) for _ in range(num_clients)]\n",
    "    num_client_samples = [len(cd.Y) for cd in all_client_data]\n",
    "\n",
    "    for r in (pbar := trange(num_rounds)):\n",
    "        losses = []\n",
    "        for client_model, client_data in zip(client_models, all_client_data):\n",
    "            client_model.set_weights(global_model.get_weights())\n",
    "            history = client_model.fit(client_data.X, client_data.Y, batch_size=batch_size, verbose=0)\n",
    "            losses.append(history.history['loss'][0])\n",
    "        global_model.set_weights(\n",
    "            optimiser([cm.get_weights() for cm in client_models], num_client_samples, global_model.get_weights())\n",
    "        )\n",
    "        pbar.set_postfix_str(f\"Loss: {np.average(losses, weights=num_client_samples):.3f}\")\n",
    "\n",
    "    evaluate(Y_test, global_model.predict(X_test, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now set up for the experiments, first we demonstrate with the same model as the centralised training and federated averaging as\n",
    "the federated optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a214e2a69554fe3b01c39d9edca8d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: -0.13630436970418502\n",
      "MAE: 0.9026521411046777\n"
     ]
    }
   ],
   "source": [
    "federated_learning(create_model, fedavg, non_iid_client_data(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this form of training is completely ineffective, so we try a different federated optimiser, FedAdam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184f05e67fe349bea5a050dd411ce598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: -0.06884925502126049\n",
      "MAE: 0.8845697449340192\n"
     ]
    }
   ],
   "source": [
    "federated_learning(create_model, FedAdam(), non_iid_client_data(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It remains arguable that this effect could simple be a side effect of the client-side Adam optimiser being used in training, and the updates\n",
    "it produces which have differing denominators causing incorrect aggregation. However, with the following we construct a closer replication of\n",
    "the centralised setting by applying the SGD optimiser at the client-side and FedAdam at the server. Still, we see that client-level\n",
    "overfitting has a significant negative impact on the resulting global model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 250/250 [01:32<00:00,  2.70it/s, Loss: 0.634]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: -0.1577248615841711\n",
      "MAE: 0.9079689140678654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_sgd_model(input_shape: Tuple[int]):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    x = inputs\n",
    "    x = tf.keras.layers.Dense(100, activation=\"sigmoid\")(x)\n",
    "    x = tf.keras.layers.Dense(1, activation=\"relu\")(x)  # All labels are positive\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(0.1), loss=\"mean_absolute_error\n",
    "    \")\n",
    "    return model\n",
    "\n",
    "federated_learning(create_sgd_model, FedAdam(), non_iid_client_data(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With i.i.d. Data\n",
    "\n",
    "With the following we show that this effect is not present when the data is i.i.d. with respect to the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083cabc9024d4023bce59c5be26de134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 21677 calls to <function Model.make_train_function.<locals>.train_function at 0x7f03421f7740> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f02d995bf60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "R2 score: 0.5511912343208749\n",
      "MAE: 0.5600718261213312\n"
     ]
    }
   ],
   "source": [
    "federated_learning(create_model, fedavg, iid_client_data(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd62c7ba4c54695911caaaa2a50762f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: -0.01119342703684234\n",
      "MAE: 0.8657854685469931\n"
     ]
    }
   ],
   "source": [
    "federated_learning(create_model, fedavg, mixed_iid_client_data(X_train, Y_train, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeModel:\n",
    "    def __init__(self, input_shape, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.parameters = np.zeros(input_shape)\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        self.parameters = weights[0]\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return [self.parameters]\n",
    "\n",
    "    def fit(self, X, Y, epochs=4, **_kwargs):\n",
    "        # The default number of epochs makes up for the fact that there are no longer minibatches\n",
    "        info = sp_opt.minimize(\n",
    "            partial(RidgeModel.func, X, Y, self.alpha),\n",
    "            x0=self.parameters,\n",
    "            method=\"L-BFGS-B\",\n",
    "            tol=1e-6,\n",
    "            bounds=[(0, np.inf)] * X.shape[1],\n",
    "            jac=True,\n",
    "            options={\"maxiter\": epochs}\n",
    "        )\n",
    "        self.parameters = info['x']\n",
    "        return namedtuple(\"History\", [\"history\"])(history={\"loss\": [info['fun'] / X.shape[1]]})\n",
    "\n",
    "    def predict(self, X, **_kwargs):\n",
    "        return X.dot(self.parameters)\n",
    "\n",
    "    def func(X, Y, alpha, w):\n",
    "        residual = X.dot(w) - Y\n",
    "        f = 0.5 * residual.dot(residual) + 0.5 * alpha * w.dot(w)\n",
    "        grad = X.T @ residual + alpha * w\n",
    "        return f, grad\n",
    "\n",
    "num_rounds = num_epochs * num_clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.4475166555450888\n",
      "MAE: 0.6637108430770596\n"
     ]
    }
   ],
   "source": [
    "model = RidgeModel(X_train[0].shape)\n",
    "model.fit(X_train, Y_train, epochs=num_epochs)\n",
    "evaluate(Y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d083de823b415b89639fdc182ca57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.4705381888755382\n",
      "MAE: 0.6504825909689994\n"
     ]
    }
   ],
   "source": [
    "federated_learning(RidgeModel, fedavg, iid_client_data(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4582988199f14f08844bc72731ed0ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: -0.11029128020171264\n",
      "MAE: 0.9635788628739557\n"
     ]
    }
   ],
   "source": [
    "federated_learning(RidgeModel, fedavg, non_iid_client_data(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c81ee300be464585d753b43e79ac70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: -0.3044178628197636\n",
      "MAE: 1.0437436436506904\n"
     ]
    }
   ],
   "source": [
    "federated_learning(RidgeModel, FedAdam(), non_iid_client_data(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c3181565db430c97b1726ad2377590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.21636184315453033\n",
      "MAE: 0.8100472070090917\n"
     ]
    }
   ],
   "source": [
    "federated_learning(RidgeModel, fedavg, mixed_iid_client_data(X_train, Y_train, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Plot trade-off between iid-proportion and performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
